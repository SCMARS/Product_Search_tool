import requests
import time
import random
import logging
from bs4 import BeautifulSoup
from typing import List, Dict, Any
import urllib.parse
from fake_useragent import UserAgent

logger = logging.getLogger(__name__)

class AllegroAdvancedScraper:
    def __init__(self):
        self.base_url = "https://allegro.pl"
        self.ua = UserAgent()
        self.session = requests.Sessistarton()
        
        # –°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
        self.proxies_list = [
            None,  # –ë–µ–∑ –ø—Ä–æ–∫—Å–∏
            # –î–æ–±–∞–≤—å—Ç–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
        ]
        
        # –°–ø–∏—Å–æ–∫ User-Agent'–æ–≤
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0'
        ]
    
    def get_headers(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pl-PL,pl;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    def search_products(self, query: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ö–æ–¥–∞"""
        products = []
        
        try:
            # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            encoded_query = urllib.parse.quote_plus(query)
            search_url = f"{self.base_url}/listing?string={encoded_query}"
            
            logger.info(f"üîç –ü–æ–∏—Å–∫ –Ω–∞ Allegro: {query}")
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            for attempt in range(3):
                try:
                    headers = self.get_headers()
                    
                    # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    time.sleep(random.uniform(1, 3))
                    
                    response = self.session.get(search_url, headers=headers, timeout=15)
                    
                    if response.status_code == 200:
                        products = self.parse_products(response.text, query, max_results)
                        if products:
                            logger.info(f"‚úÖ Allegro: –Ω–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
                            return products
                    
                    elif response.status_code == 403:
                        logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –î–æ—Å—Ç—É–ø –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π –º–µ—Ç–æ–¥")
                        time.sleep(random.uniform(3, 7))
                        continue
                    
                    else:
                        logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –°—Ç–∞—Ç—É—Å {response.status_code}")
                        
                except Exception as e:
                    logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞: {e}")
                    time.sleep(random.uniform(2, 5))
            
            # –ï—Å–ª–∏ –æ–±—ã—á–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
            return self.try_mobile_version(query, max_results)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ Allegro: {e}")
            return []
    
    def try_mobile_version(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """–ü–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ –º–æ–±–∏–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é"""
        try:
            encoded_query = urllib.parse.quote_plus(query)
            mobile_url = f"https://m.allegro.pl/listing?string={encoded_query}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'pl-PL,pl;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br'
            }
            
            response = self.session.get(mobile_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                products = self.parse_mobile_products(response.text, query, max_results)
                if products:
                    logger.info(f"‚úÖ Allegro Mobile: –Ω–∞–π–¥–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
                    return products
            
        except Exception as e:
            logger.warning(f"–ú–æ–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
        
        return []
    
    def parse_products(self, html: str, query: str, max_results: int) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            products = []
            
            # –ò—â–µ–º —Ç–æ–≤–∞—Ä—ã –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            selectors = [
                'article[data-role="offer"]',
                'div[data-role="offer"]',
                '.offer-item',
                '.listing-item',
                '[data-testid="listing-item"]'
            ]
            
            items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    break
            
            if not items:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã –≤ HTML")
                return []
            
            for item in items[:max_results]:
                try:
                    product = self.extract_product_info(item)
                    if product and self.is_relevant(product['name'], query):
                        products.append(product)
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {e}")
                    continue
            
            return products
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML: {e}")
            return []
    
    def parse_mobile_products(self, html: str, query: str, max_results: int) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            products = []
            
            # –ú–æ–±–∏–ª—å–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            items = soup.select('.listing-item, .offer-item, [data-testid="listing-item"]')
            
            for item in items[:max_results]:
                try:
                    product = self.extract_mobile_product_info(item)
                    if product and self.is_relevant(product['name'], query):
                        products.append(product)
                except Exception as e:
                    continue
            
            return products
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏: {e}")
            return []
    
    def extract_product_info(self, item) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–µ"""
        try:
            # –ù–∞–∑–≤–∞–Ω–∏–µ
            name_selectors = [
                'h2 a', '.offer-title a', '[data-testid="listing-item-title"] a',
                '.listing-item-title a', 'a[data-testid="listing-item-title"]'
            ]
            name = ""
            link = ""
            
            for selector in name_selectors:
                name_elem = item.select_one(selector)
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    link = name_elem.get('href', '')
                    break
            
            if not name:
                return None
            
            # –¶–µ–Ω–∞
            price_selectors = [
                '.price', '.offer-price', '[data-testid="listing-item-price"]',
                '.listing-item-price', '.price-value'
            ]
            price = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
            
            for selector in price_selectors:
                price_elem = item.select_one(selector)
                if price_elem:
                    price = price_elem.get_text(strip=True)
                    break
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_selectors = [
                'img', '.offer-image img', '[data-testid="listing-item-image"] img'
            ]
            image = ""
            
            for selector in img_selectors:
                img_elem = item.select_one(selector)
                if img_elem:
                    image = img_elem.get('src', '') or img_elem.get('data-src', '')
                    break
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
            if link and not link.startswith('http'):
                link = f"https://allegro.pl{link}"
            
            return {
                'name': name,
                'price': price,
                'url': link,
                'image': image,
                'description': name,
                'source': 'allegro'
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return None
    
    def extract_mobile_product_info(self, item) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–æ–≤–∞—Ä–µ –∏–∑ –º–æ–±–∏–ª—å–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
        return self.extract_product_info(item)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥
    
    def is_relevant(self, title: str, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–≤–∞—Ä–∞"""
        if not title or not query:
            return False
        
        title_lower = title.lower()
        query_lower = query.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_words = query_lower.split()
        matched_words = sum(1 for word in query_words if word in title_lower)
        
        # –¢–æ–≤–∞—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –µ—Å–ª–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Ö–æ—Ç—è –±—ã –ø–æ–ª–æ–≤–∏–Ω–∞ —Å–ª–æ–≤
        return matched_words >= len(query_words) * 0.5

def search_allegro_advanced(query: str, max_results: int = 10) -> List[Dict[str, Any]]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ Allegro"""
    scraper = AllegroAdvancedScraper()
    return scraper.search_products(query, max_results)
