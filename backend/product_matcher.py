

import pandas as pd
import logging
import re
import json
from typing import Dict, List, Optional, Tuple
from rapidfuzz import fuzz
import time
import os
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –º–æ–¥—É–ª–∏ –ø–æ–∏—Å–∫–∞
from amazon import search_amazon
from aliexpress import search_aliexpress
from allegro import search_allegro_improved

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductMatcher:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º –∏–∑ —Ñ–∞–π–ª–∞
    """
    
    def __init__(self):
        self.results = []
        self.processed_count = 0
        self.total_count = 0
        
    def read_file(self, file_path: str) -> pd.DataFrame:
        """
        –ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª CSV –∏–ª–∏ Excel –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame
        """
        try:
            if file_path.lower().endswith('.csv'):
                df = pd.read_csv(file_path, encoding='utf-8')
            elif file_path.lower().endswith(('.xlsx', '.xls')):
                df = pd.read_excel(file_path)
            else:
                raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã CSV –∏ Excel")
                
            logger.info(f"–§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(df)} —Å—Ç—Ä–æ–∫, –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            return df
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            raise
    
    def build_search_query(self, row: pd.Series) -> str:


        query_parts = []
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
        priority_fields = [
            'brand', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞',
            'product_name', '–Ω–∞–∑–≤–∞–Ω–∏–µ', 'name', 'product',
            'model', '–º–æ–¥–µ–ª—å',
            'category', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è',
            'type', '—Ç–∏–ø'
        ]
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        additional_fields = [
            'color', '—Ü–≤–µ—Ç', 'colour',
            'size', '—Ä–∞–∑–º–µ—Ä',
            'material', '–º–∞—Ç–µ—Ä–∏–∞–ª',
            'keywords', '–∫–ª—é—á–µ–≤—ã–µ_—Å–ª–æ–≤–∞', 'tags'
        ]
        
        # –°–æ–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        for field in priority_fields:
            if field in row.index and pd.notna(row[field]) and str(row[field]).strip():
                value = str(row[field]).strip()
                if value.lower() not in ['nan', 'null', '']:
                    query_parts.append(value)
                    
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        for field in additional_fields:
            if field in row.index and pd.notna(row[field]) and str(row[field]).strip():
                value = str(row[field]).strip()
                if value.lower() not in ['nan', 'null', '']:
                    query_parts.append(value)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        query = ' '.join(query_parts[:5])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–ø—Ä–æ—Å–∞
        
        # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        query = re.sub(r'[^\w\s\-]', ' ', query)
        query = re.sub(r'\s+', ' ', query).strip()
        
        logger.info(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∑–∞–ø—Ä–æ—Å: '{query}'")
        return query
    
    def extract_characteristics(self, row: pd.Series) -> Dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        characteristics = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        char_mapping = {
            'brand': ['brand', '–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞'],
            'color': ['color', '—Ü–≤–µ—Ç', 'colour'],
            'size': ['size', '—Ä–∞–∑–º–µ—Ä'],
            'material': ['material', '–º–∞—Ç–µ—Ä–∏–∞–ª'],
            'category': ['category', '–∫–∞—Ç–µ–≥–æ—Ä–∏—è'],
            'type': ['type', '—Ç–∏–ø'],
            'model': ['model', '–º–æ–¥–µ–ª—å']
        }
        
        for char_key, possible_fields in char_mapping.items():
            for field in possible_fields:
                if field in row.index and pd.notna(row[field]):
                    value = str(row[field]).strip()
                    if value.lower() not in ['nan', 'null', '']:
                        characteristics[char_key] = value.lower()
                        break
        
        return characteristics
    
    def calculate_relevance_score(self, product: Dict, query: str, characteristics: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        """
        score = 0.0
        
        title = product.get('name', '').lower()
        description = product.get('description', '').lower()
        
        # –ë–∞–∑–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (40% –≤–µ—Å–∞)
        title_score = fuzz.partial_ratio(query.lower(), title) / 100.0
        score += title_score * 0.4
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é (20% –≤–µ—Å–∞)
        if description:
            desc_score = fuzz.partial_ratio(query.lower(), description) / 100.0
            score += desc_score * 0.2
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (40% –≤–µ—Å–∞)
        char_bonus = 0.0
        char_count = 0
        
        for char_key, char_value in characteristics.items():
            if char_value:
                char_count += 1
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–∏
                if char_value in title or char_value in description:
                    char_bonus += 1.0
                elif any(word in title for word in char_value.split()):
                    char_bonus += 0.5
        
        if char_count > 0:
            score += (char_bonus / char_count) * 0.4
        
        return min(score, 1.0)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä
    
    def filter_relevant_products(self, products: List[Dict], query: str, 
                                characteristics: Dict, min_score: float = 0.3) -> List[Dict]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–≤–∞—Ä—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        scored_products = []
        
        for product in products:
            score = self.calculate_relevance_score(product, query, characteristics)
            if score >= min_score:
                product['relevance_score'] = score
                scored_products.append(product)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_products.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(scored_products)} –∏–∑ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
        return scored_products[:10]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10
    
    def search_single_product(self, row: pd.Series, row_index: int) -> Dict:
        """
        –ò—â–µ—Ç —Ç–æ–≤–∞—Ä –ø–æ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –∏–∑ —Ñ–∞–π–ª–∞
        """
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ {row_index + 1}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        query = self.build_search_query(row)
        if not query:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å—Ç—Ä–æ–∫–∏ {row_index + 1}")
            return {
                'row_index': row_index + 1,
                'query': '',
                'error': '–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å',
                'amazon': [],
                'aliexpress': [],
                'allegro': []
            }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        characteristics = self.extract_characteristics(row)
        
        result = {
            'row_index': row_index + 1,
            'query': query,
            'characteristics': characteristics,
            'amazon': [],
            'aliexpress': [],
            'allegro': []
        }
        
        # –ü–æ–∏—Å–∫ –Ω–∞ Amazon
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–∞ Amazon: {query}")
            amazon_products = search_amazon(query, max_pages=1)
            filtered_amazon = self.filter_relevant_products(amazon_products, query, characteristics)
            result['amazon'] = filtered_amazon
            logger.info(f"Amazon: –Ω–∞–π–¥–µ–Ω–æ {len(filtered_amazon)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ Amazon: {e}")
            result['amazon_error'] = str(e)
        
        # –ü–æ–∏—Å–∫ –Ω–∞ AliExpress
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–∞ AliExpress: {query}")
            aliexpress_products = search_aliexpress(query, limit=10)
            filtered_aliexpress = self.filter_relevant_products(aliexpress_products, query, characteristics)
            result['aliexpress'] = filtered_aliexpress
            logger.info(f"AliExpress: –Ω–∞–π–¥–µ–Ω–æ {len(filtered_aliexpress)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ AliExpress: {e}")
            result['aliexpress_error'] = str(e)
        
        # –ü–æ–∏—Å–∫ –Ω–∞ Allegro —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        try:
            logger.info(f"–ü–æ–∏—Å–∫ –Ω–∞ Allegro: {query}")
            allegro_products = search_allegro_improved(query, max_pages=1)
            filtered_allegro = self.filter_relevant_products(allegro_products, query, characteristics)
            result['allegro'] = filtered_allegro
            logger.info(f"Allegro: –Ω–∞–π–¥–µ–Ω–æ {len(filtered_allegro)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ Allegro: {e}")
            result['allegro_error'] = str(e)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        time.sleep(2)
        
        return result

    def process_file(self, file_path: str, output_file: str = None) -> List[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
        """
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if output_file and os.path.exists(output_file):
            os.remove(output_file)
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {output_file}")

        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        df = self.read_file(file_path)
        self.total_count = len(df)
        self.processed_count = 0
        self.results = []

        logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {self.total_count} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
        for index, row in df.iterrows():
            try:
                result = self.search_single_product(row, index)
                self.results.append(result)
                self.processed_count += 1

                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.processed_count}/{self.total_count} —Ç–æ–≤–∞—Ä–æ–≤")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {index + 1}: {e}")
                error_result = {
                    'row_index': index + 1,
                    'error': str(e),
                    'amazon': [],
                    'aliexpress': [],
                    'allegro': []
                }
                self.results.append(error_result)
                self.processed_count += 1

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if output_file:
            self.save_results(output_file)

        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.processed_count} —Ç–æ–≤–∞—Ä–æ–≤")
        return self.results

    def save_results(self, output_file: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            output_data = {
                'metadata': {
                    'processed_at': datetime.now().isoformat(),
                    'total_products': self.total_count,
                    'processed_products': self.processed_count,
                    'success_rate': f"{(self.processed_count / self.total_count * 100):.1f}%" if self.total_count > 0 else "0%"
                },
                'results': self.results
            }

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")

    def export_to_excel(self, output_file: str):
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel —Ñ–∞–π–ª
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è Excel
            excel_data = []

            for result in self.results:
                row_data = {
                    '–°—Ç—Ä–æ–∫–∞': result.get('row_index', ''),
                    '–ó–∞–ø—Ä–æ—Å': result.get('query', ''),
                    '–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏': json.dumps(result.get('characteristics', {}), ensure_ascii=False),
                    '–û—à–∏–±–∫–∞': result.get('error', '')
                }

                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –ø–ª–æ—â–∞–¥–∫–µ
                for platform in ['amazon', 'aliexpress', 'allegro']:
                    products = result.get(platform, [])
                    if products:
                        # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        best_product = products[0]
                        row_data[f'{platform.title()}_–ù–∞–∑–≤–∞–Ω–∏–µ'] = best_product.get('name', '')
                        row_data[f'{platform.title()}_–¶–µ–Ω–∞'] = best_product.get('price', '')
                        row_data[f'{platform.title()}_URL'] = best_product.get('url', '')
                        row_data[f'{platform.title()}_–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å'] = f"{best_product.get('relevance_score', 0):.2f}"
                    else:
                        row_data[f'{platform.title()}_–ù–∞–∑–≤–∞–Ω–∏–µ'] = ''
                        row_data[f'{platform.title()}_–¶–µ–Ω–∞'] = ''
                        row_data[f'{platform.title()}_URL'] = ''
                        row_data[f'{platform.title()}_–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å'] = ''

                excel_data.append(row_data)

            # –°–æ–∑–¥–∞–µ–º DataFrame –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            df = pd.DataFrame(excel_data)
            df.to_excel(output_file, index=False, engine='openpyxl')

            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ Excel: {output_file}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel: {e}")

    def get_progress(self) -> Dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        return {
            'total': self.total_count,
            'processed': self.processed_count,
            'percentage': (self.processed_count / self.total_count * 100) if self.total_count > 0 else 0
        }


def create_sample_csv(filename: str = "sample_products.csv"):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä CSV —Ñ–∞–π–ª–∞ —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤
    """
    sample_data = [
        {
            'product_name': 'iPhone 15 Pro',
            'brand': 'Apple',
            'category': '–°–º–∞—Ä—Ç—Ñ–æ–Ω—ã',
            'color': '–ß–µ—Ä–Ω—ã–π',
            'size': '128GB',
            'keywords': '—Ç–µ–ª–µ—Ñ–æ–Ω –º–æ–±–∏–ª—å–Ω—ã–π –∞–π—Ñ–æ–Ω'
        },
        {
            'product_name': 'MacBook Air',
            'brand': 'Apple',
            'category': '–ù–æ—É—Ç–±—É–∫–∏',
            'color': '–°–µ—Ä–µ–±—Ä–∏—Å—Ç—ã–π',
            'size': '13 –¥—é–π–º–æ–≤',
            'keywords': '–Ω–æ—É—Ç–±—É–∫ –ª—ç–ø—Ç–æ–ø –º–∞–∫–±—É–∫'
        },
        {
            'product_name': '–ö–æ—Ñ–µ–º–∞—à–∏–Ω–∞',
            'brand': 'DeLonghi',
            'category': '–ë—ã—Ç–æ–≤–∞—è —Ç–µ—Ö–Ω–∏–∫–∞',
            'type': '–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è',
            'keywords': '–∫–æ—Ñ–µ —ç—Å–ø—Ä–µ—Å—Å–æ –∫–∞–ø—É—á–∏–Ω–æ'
        }
    ]

    df = pd.DataFrame(sample_data)
    df.to_csv(filename, index=False, encoding='utf-8')
    print(f"–°–æ–∑–¥–∞–Ω –ø—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞: {filename}")


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    matcher = ProductMatcher()

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä —Ñ–∞–π–ª–∞
    create_sample_csv()

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
    results = matcher.process_file("sample_products.csv", "results_detailed.json")

    # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤ Excel
    matcher.export_to_excel("results_detailed.xlsx")

    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results_detailed.json –∏ results_detailed.xlsx")
